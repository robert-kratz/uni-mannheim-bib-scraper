{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oXC3fRchMc3"
      },
      "source": [
        "# Roadmap\n",
        "## [DONE] Create primitive model, predicting for only one library (A3)\n",
        "## Create model for all libraries\n",
        "Here we can:\n",
        "1. for-loops to create datasets and train separate models (might be computationally expensive)\n",
        "2. **multi-input: whole dataset + encoded library name => BEST COMPROMISE (faster than the others bc only one model trained and no need to model inter-dependencies => good if we get 5% mae)**\n",
        "3. data of one library + data of all libraries as inputs => better prediction for one library that accounts for inter-dependencies, BUT extremely computationally expensive (more inputs and more models)\n",
        "\n",
        "## Tweak model to improve its accuracy\n",
        "## Create pipeline for receiving data in real-time, once per hour  \n",
        "## 'Integrate' model into the website"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZGqzzwUzrNz"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EGw5IDX0b2I"
      },
      "outputs": [],
      "source": [
        "import tracemalloc\n",
        "tracemalloc.start()\n",
        "\n",
        "import sqlite3\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, GRU, Dense, Concatenate, Dropout, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# # Mount drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Connect to the database\n",
        "#db_path = \"/content/drive/MyDrive/BibScraperModel/dev.db\"\n",
        "\n",
        "db_path = \"../prisma/dev.db\"\n",
        "conn = sqlite3.connect(db_path)\n",
        "\n",
        "# Load pre-trained RNN model???\n",
        "# model = load_model(\"path_to_your_model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxswYn0pHItg"
      },
      "source": [
        "### Check that Google Colab takes the database accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-58zad-4iHn"
      },
      "outputs": [],
      "source": [
        "#!ls \"/content/drive/MyDrive/BibScraperModel\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2lf-2Wq5kMa"
      },
      "outputs": [],
      "source": [
        "# cursor = conn.cursor()\n",
        "# cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "# print(cursor.fetchall())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Gq5LMQnHNOz"
      },
      "source": [
        "### Fetch the data function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBzCX_Nv0b2O"
      },
      "outputs": [],
      "source": [
        "def fetch_latest_data():\n",
        "    query = f\"\"\"\n",
        "        SELECT name, year, month, day, chunk, percentage\n",
        "        FROM BibData\n",
        "    \"\"\"\n",
        "    df = pd.read_sql(query, conn)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NINl5_T0b2P"
      },
      "source": [
        "### DATA FETCHING IDEA: TAKE DIRECTLY ONLY THE DATA FROM THE LAST 1 month or so\n",
        "storage optimization, time efficiency purposes  \n",
        "model could (theoretically) still be good with a dataset of 1 month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mp0umAIM0b2Q"
      },
      "outputs": [],
      "source": [
        "df = fetch_latest_data()\n",
        "df = df.drop_duplicates().reset_index(drop=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_figSBmzTKD"
      },
      "source": [
        "### Preprocess data into a dictionary with encoded libraries and their occupancy percentages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3t8-T1ChasRA"
      },
      "outputs": [],
      "source": [
        "# Sort the DataFrame by Library, chronologically\n",
        "df = df.sort_values(by=['name', 'year', 'month', 'day', 'chunk']).reset_index(drop=True)\n",
        "\n",
        "# One-hot encode the library names\n",
        "library_names = df['name'].unique().reshape(-1, 1)\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "one_hot_keys = encoder.fit_transform(library_names)\n",
        "\n",
        "# Create the dictionary with one-hot encoded keys\n",
        "data_by_library = {\n",
        "    tuple(one_hot): df[df['name'] == library].drop(columns=['name', 'year', 'month', 'day', 'chunk'])\n",
        "    for one_hot, library in zip(one_hot_keys, library_names.flatten())\n",
        "}\n",
        "\n",
        "# Normalize percentage values with min max scaler\n",
        "# Because of floating-point operation errors, we round\n",
        "for library, data in data_by_library.items():\n",
        "  scaler = MinMaxScaler()\n",
        "  data['Occupancy'] = scaler.fit_transform(data['percentage'].values.reshape(-1, 1)).round(2)\n",
        "  data = data.drop(columns=['percentage'])\n",
        "\n",
        "  # Update the dictionary with the changed DataFrame\n",
        "  data_by_library[library] = data\n",
        "\n",
        "data_by_library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGUzDoT50b2U"
      },
      "source": [
        "### Prepare data for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahSUsurh0b2T"
      },
      "outputs": [],
      "source": [
        "# We look 5 days in the past (5 days x 24 hours x 6 chunks = 720 chunks)\n",
        "past = 720\n",
        "\n",
        "# We want to predict the next day (1 day x 24 hours x 6 chunks = 144 chunks)\n",
        "future = 144\n",
        "\n",
        "# We sample data every hour - look at it every 6 chunks within the (past, future) timeframe\n",
        "# We do this to reduce the amount of data to process to a manageable size\n",
        "sampling_rate = 6\n",
        "\n",
        "# Define the sequence length:\n",
        "# We actually look at 720 / 6 = 120 timesteps in the past (120 points of past data)\n",
        "sequence_length = int(past / sampling_rate)\n",
        "\n",
        "# Same for the future steps:\n",
        "# We actually look at 144 / 6 = 24 timesteps in the future (24 hours)\n",
        "future_steps = int(future / sampling_rate)\n",
        "\n",
        "# Save number of libraries\n",
        "num_libraries = len(encoder.categories_[0])  # Number of unique libraries\n",
        "\n",
        "# 80% train, 20% validation\n",
        "# Note that there is no test data, since we do not actually know the future values to test against\n",
        "split_fraction = 0.8\n",
        "\n",
        "# Get train split index for all dataframes\n",
        "train_split = [int(split_fraction * len(df)) for df in data_by_library.values()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRUARBOQxNaz"
      },
      "outputs": [],
      "source": [
        "### REFERENCE THE CODE HERE IF REVERTING TO AUTOMATIC DATASET CREATION\n",
        "# # in the end, we only need the percentage data for the model\n",
        "# # because the order of the data, sorted chronologically, already encodes the time dependency\n",
        "# data = df['Percentage'].values\n",
        "\n",
        "# x_train = data[: train_split]\n",
        "# y_train = data[past : train_split + future]\n",
        "\n",
        "# x_val = data[train_split : len(data) - future] # don't go to the end, let the future data be the target\n",
        "# y_val = data[train_split + past :] # offset train_split by future"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk-ltyiOwEod"
      },
      "outputs": [],
      "source": [
        "# Train and validation data containers\n",
        "train_sequences = []\n",
        "val_sequences = []\n",
        "train_library_inputs = []\n",
        "val_library_inputs = []\n",
        "train_targets = []\n",
        "val_targets = []\n",
        "\n",
        "# Process each library\n",
        "for (library_key, data), train_idx in zip(data_by_library.items(), train_split):\n",
        "\n",
        "    # Split into training and validation sets\n",
        "    occupancy = data['Occupancy'].values\n",
        "    train_values = occupancy[:train_idx]\n",
        "    val_values = occupancy[train_idx:]\n",
        "\n",
        "    # Generate training sequences\n",
        "    for i in range(0, len(train_values) - past - future, sampling_rate):\n",
        "        # Input sequence for past data\n",
        "        train_sequences.append(train_values[i:i + past:sampling_rate])\n",
        "        train_library_inputs.append(library_key)\n",
        "        # Target sequence for future data\n",
        "        train_targets.append(train_values[i + past : i + past + future:sampling_rate])\n",
        "\n",
        "    # Generate validation sequences\n",
        "    for i in range(0, len(val_values) - past - future, sampling_rate):\n",
        "        # Input sequence for past data\n",
        "        val_sequences.append(val_values[i:i + past:sampling_rate])\n",
        "        val_library_inputs.append(library_key)\n",
        "        # Target sequence for future data\n",
        "        val_targets.append(val_values[i + past : i + past + future:sampling_rate])\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "train_sequences = np.array(train_sequences).reshape(-1, sequence_length, 1)\n",
        "val_sequences = np.array(val_sequences).reshape(-1, sequence_length, 1)\n",
        "train_library_inputs = np.array(train_library_inputs)\n",
        "val_library_inputs = np.array(val_library_inputs)\n",
        "train_targets = np.array(train_targets)\n",
        "val_targets = np.array(val_targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWTDeB3syzUZ"
      },
      "source": [
        "### Build the multi-input model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XddZNHv50b2V"
      },
      "outputs": [],
      "source": [
        "# Define multi-input GRU model\n",
        "def build_multi_input_model(sequence_length, num_libraries, future_steps):\n",
        "    seq_input = Input(shape=(sequence_length, 1), name=\"sequence_input\")\n",
        "\n",
        "    x = GRU(128, activation=\"tanh\", return_sequences=True)(seq_input)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = GRU(48, activation=\"tanh\", return_sequences=False)(x)\n",
        "\n",
        "    lib_input = Input(shape=(num_libraries,), name=\"library_input\")\n",
        "    combined = Concatenate()([x, lib_input])\n",
        "\n",
        "    x = Dense(128, activation=\"relu\")(combined)\n",
        "    output = Dense(future_steps, name=\"output\")(x)\n",
        "\n",
        "    model = Model(inputs=[seq_input, lib_input], outputs=output)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\", metrics=[\"mae\"])\n",
        "    return model\n",
        "\n",
        "model = build_multi_input_model(sequence_length, num_libraries, future_steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rv3-nxbH0b2W"
      },
      "outputs": [],
      "source": [
        "# Define callbacks\n",
        "# UNCOMMENT IF RUNNING ON MORE EPOCHS\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
        "\n",
        "# learning_rate_reduction = ReduceLROnPlateau(\n",
        "#     monitor='val_loss', factor=0.1, patience=2, min_lr=1e-5\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tMsEmqHz2Ir"
      },
      "source": [
        "### Train and validate\n",
        "(!!!) If we deploy this and have a TPU at disposal: crank the learning rate up to 0.01, put 20 epochs, add the callbacks and maybe adjust batch size\n",
        "Also, maybe adjust 'past' parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qg-_WPUL0b2W"
      },
      "outputs": [],
      "source": [
        "# RNNs usually slow, computationally expensive:\n",
        "# calculations are done sequentially, it all depends on the previous output => no parallelization possible\n",
        "\n",
        "model.fit(\n",
        "    [train_sequences, train_library_inputs],\n",
        "    train_targets,\n",
        "    validation_data=([val_sequences, val_library_inputs], val_targets),\n",
        "    epochs=5,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stopping, learning_rate_reduction]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfrzUCfNu2If"
      },
      "outputs": [],
      "source": [
        "# Get current and peak memory usage\n",
        "current, peak = tracemalloc.get_traced_memory()\n",
        "print(f\"Current memory usage: {current / 1024 ** 2:.2f} MB\")\n",
        "print(f\"Peak memory usage: {peak / 1024 ** 2:.2f} MB\")\n",
        "\n",
        "# Stop tracing\n",
        "tracemalloc.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dlenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
